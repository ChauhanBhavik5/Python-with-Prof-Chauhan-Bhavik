{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f287a07",
   "metadata": {},
   "source": [
    "# Wrangling Data with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa7489",
   "metadata": {},
   "source": [
    "### 1) Concept of Hashing in Data Processing\n",
    "\n",
    "- **Hashing** = applying a hash function to data (like words or IDs) to convert them into fixed-size numerical values.\n",
    "\n",
    "- A hash function takes input (e.g., a word like \"apple\") and outputs a number.\n",
    "\n",
    "- The output number is within a fixed range (say 0–999 if we set 1000 buckets).\n",
    "\n",
    "- This is very useful for handling large, high-dimensional data (like text, logs, categorical values).\n",
    "\n",
    "#### Key properties of hashing in data processing:\n",
    "\n",
    "- Deterministic → same input always gives the same output.\n",
    "\n",
    "- \"apple\" will always map to the same number.\n",
    "\n",
    "- Fixed range → regardless of dataset size, results fit into a fixed number of buckets.\n",
    "\n",
    "- Fast & memory-efficient → avoids creating huge feature dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019eb5d",
   "metadata": {},
   "source": [
    "#### Example (conceptual):\n",
    "\n",
    "- Suppose we hash words into 10 buckets:\n",
    "\n",
    "- **Word** → **Hash Value** → **Bucket**\n",
    "\n",
    "    \"apple\" → 232 → 2\n",
    "\n",
    "    \"banana\" → 546 → 6\n",
    "\n",
    "    \"car\" → 832 → 2 (collision with apple!)\n",
    "    \n",
    "\n",
    "- Sometimes different words land in the same bucket → called a collision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6612bb",
   "metadata": {},
   "source": [
    "### 2) Why and When to Use Hashing in Machine Learning\n",
    "\n",
    "#### Why use it?\n",
    "\n",
    "- **Scalability:** Works with very large vocabularies (millions of words).\n",
    "\n",
    "- **Speed:** Faster than building large dictionaries.\n",
    "\n",
    "- **Low Memory Usage:** Fixed-size representation saves RAM.\n",
    "\n",
    "- **Streaming Data:** Useful when new features appear continuously (no need to rebuild dictionary).\n",
    "\n",
    "#### When to use it?\n",
    "\n",
    "- Text data (like tweets, reviews, logs) → feature extraction without storing the whole vocabulary.\n",
    "\n",
    "- Large categorical variables (like millions of unique IDs, products, or URLs).\n",
    "\n",
    "- Online learning systems where data arrives in a stream and vocabulary keeps changing.\n",
    "\n",
    "#### Example (Text Classification Scenario)\n",
    "\n",
    "- Without hashing:\n",
    "\n",
    "    - We must store a dictionary:\n",
    "    { \"apple\":0, \"banana\":1, \"car\":2, ... }\n",
    "\n",
    "- With hashing:\n",
    "    - We skip the dictionary.\n",
    "    - Each word is directly hashed into a number → mapped into a fixed-size vector.\n",
    "    \n",
    "    \n",
    "### Summary:\n",
    "\n",
    "- Hashing = mapping input data to fixed-size numeric buckets using a hash function.\n",
    "\n",
    "- Advantage: fast, memory-efficient, scalable for huge text or categorical datasets.\n",
    "\n",
    "- Used in ML when vocab/feature space is very large or streaming.\n",
    "\n",
    "- Limitation: collisions (different items map to same bucket) → but usually acceptable in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a1b51",
   "metadata": {},
   "source": [
    "## Using Hash Functions\n",
    "### 3) Properties of Hash Functions\n",
    "\n",
    "When we use hashing in data processing or ML, we need hash functions with certain properties:\n",
    "\n",
    "##### 1) Determinism \n",
    "\n",
    "- The same input always produces the same output.\n",
    "\n",
    "- Example: hash(\"apple\") → always the same value (in the same Python session).\n",
    "\n",
    "##### 2) Uniformity \n",
    "\n",
    "- Hash values should be evenly distributed across the range (buckets).\n",
    "\n",
    "- Avoids “clustering” where too many items land in the same bucket.\n",
    "\n",
    "##### 3) Speed \n",
    "\n",
    "- Hashing should be fast to compute since it is used for very large datasets.\n",
    "- Slower functions (like cryptographic hashes) are secure but not efficient for ML wrangling.\n",
    "\n",
    "##### 4) Fixed Range (Modulus Operation)\n",
    "\n",
    "- Usually we use modulus (% N) to keep values in a fixed range of buckets.\n",
    "\n",
    "##### Important Note:\n",
    "- In machine learning, we usually prefer fast and simple hash functions (not cryptographic ones like SHA256)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294eb99c",
   "metadata": {},
   "source": [
    "### 4) Examples of Hash Functions in Python\n",
    "#### A. Built-in hash() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0411f3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5445322363184041899\n",
      "-8279985716640786902\n",
      "5190923733118357545\n"
     ]
    }
   ],
   "source": [
    "# Python built-in hash function\n",
    "print(hash(\"apple\"))\n",
    "print(hash(\"banana\"))\n",
    "print(hash(\"car\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b996b",
   "metadata": {},
   "source": [
    "- **Note:** Python’s built-in hash() changes between runs (for security reasons)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f07a90",
   "metadata": {},
   "source": [
    "#### B. Using hashlib (Cryptographic Hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d8cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA256: 3a7bd3e2360a3d29eea436fcfb7e44c735d117c42d1c1835420b6b9942dd4f1b\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# SHA256 hash\n",
    "word = \"apple\"\n",
    "hash_value = hashlib.sha256(word.encode()).hexdigest()\n",
    "print(\"SHA256:\", hash_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148f181",
   "metadata": {},
   "source": [
    "- Cryptographic hashes (MD5, SHA1, SHA256) are deterministic, uniform, secure but slower.\n",
    "- Used in security/forensics, not usually in ML feature hashing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61b473",
   "metadata": {},
   "source": [
    "#### C. Using Scikit-learn’s Hashing for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d622ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed Feature Matrix:\n",
      " [[ 0.         -0.57735027  0.         -0.57735027  0.          0.57735027\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.37796447  0.          0.          0.          0.          0.37796447\n",
      "   0.         -0.37796447  0.         -0.75592895]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "corpus = [\"I love data science\", \"Data wrangling with scikit-learn\"]\n",
    "\n",
    "hv = HashingVectorizer(n_features=10)  # 10 buckets\n",
    "X = hv.fit_transform(corpus)\n",
    "\n",
    "print(\"Hashed Feature Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deffa70",
   "metadata": {},
   "source": [
    "Here, each word is hashed directly into one of 10 buckets.\n",
    "\n",
    "- No need to store vocabulary.\n",
    "\n",
    "- Useful for text processing with large data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc59425",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Hash functions must be deterministic, uniform, and fast.\n",
    "\n",
    "- Python offers:\n",
    "\n",
    "    - hash() (fast but changes across runs)\n",
    "    - hashlib (cryptographic, slower, more secure)\n",
    "    - HashingVectorizer in Scikit-learn (practical for ML text wrangling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de82e7",
   "metadata": {},
   "source": [
    "## Demonstrating the Hashing Trick\n",
    "### 5) Implementing the Hashing Trick in Scikit-learn\n",
    "\n",
    "Scikit-learn provides HashingVectorizer to apply the hashing trick directly.\n",
    "\n",
    "#### HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f478f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed Feature Matrix:\n",
      " [[0.         0.57735027 0.         0.57735027 0.         0.57735027\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.37796447 0.         0.         0.         0.         0.37796447\n",
      "  0.         0.37796447 0.         0.75592895]\n",
      " [0.4472136  0.         0.4472136  0.         0.4472136  0.4472136\n",
      "  0.4472136  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Example text corpus\n",
    "corpus = [\n",
    "    \"I love data science\",\n",
    "    \"Data wrangling with scikit-learn\",\n",
    "    \"Machine learning needs clean data\"\n",
    "]\n",
    "\n",
    "# Apply hashing trick\n",
    "hv = HashingVectorizer(n_features=10, alternate_sign=False)  # 10 buckets\n",
    "X_hash = hv.fit_transform(corpus)\n",
    "\n",
    "print(\"Hashed Feature Matrix:\\n\", X_hash.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959036b9",
   "metadata": {},
   "source": [
    "- Each word is hashed into one of 10 buckets (features).\n",
    "\n",
    "- Vocabulary is not stored → very memory-efficient.\n",
    "\n",
    "- alternate_sign=False ensures only positive values.\n",
    "\n",
    "- Collision possible (different words → same bucket)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985c1b7",
   "metadata": {},
   "source": [
    "### 6) Comparing with Traditional Vectorization\n",
    "#### A. CountVectorizer (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ba841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['clean' 'data' 'learn' 'learning' 'love' 'machine' 'needs' 'science'\n",
      " 'scikit' 'with' 'wrangling']\n",
      "Count Vectorizer Matrix:\n",
      " [[0 1 0 0 1 0 0 1 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 1 1 1]\n",
      " [1 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_count = cv.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", cv.get_feature_names_out())\n",
    "print(\"Count Vectorizer Matrix:\\n\", X_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41398e3d",
   "metadata": {},
   "source": [
    "- Creates a dictionary of words → indices.\n",
    "\n",
    "- Good for small data, but memory-heavy for large vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a42ed",
   "metadata": {},
   "source": [
    "#### B. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6331e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['clean' 'data' 'learn' 'learning' 'love' 'machine' 'needs' 'science'\n",
      " 'scikit' 'with' 'wrangling']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.38537163 0.         0.         0.65249088 0.\n",
      "  0.         0.65249088 0.         0.         0.        ]\n",
      " [0.         0.28321692 0.47952794 0.         0.         0.\n",
      "  0.         0.         0.47952794 0.47952794 0.47952794]\n",
      " [0.47952794 0.28321692 0.         0.47952794 0.         0.47952794\n",
      "  0.47952794 0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4766b",
   "metadata": {},
   "source": [
    "- Adjusts word counts using importance (rare words get higher weight).\n",
    "\n",
    "- Better for text classification than plain counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920a558",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "- CountVectorizer: simple word counts (memory-heavy for big data).\n",
    "\n",
    "- TF-IDF: weighted counts → considers importance of words.\n",
    "\n",
    "- HashingVectorizer: no vocabulary stored, scalable, but risk of collisions.\n",
    "\n",
    "##### Rule of Thumb:\n",
    "\n",
    "- Use Count/TF-IDF for small to medium datasets.\n",
    "\n",
    "- Use HashingVectorizer for large-scale or streaming text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8f14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
