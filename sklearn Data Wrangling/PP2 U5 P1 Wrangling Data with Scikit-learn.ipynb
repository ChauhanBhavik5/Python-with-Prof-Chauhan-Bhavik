{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152a8572-32ec-48b2-b7d6-bed91b9a338d",
   "metadata": {},
   "source": [
    "# Wrangling Data with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef61bd-1bed-4d04-9bf2-3e7db4a33762",
   "metadata": {},
   "source": [
    "## 1) Introduction to Data Wrangling in Python\n",
    "\n",
    "### What is Data Wrangling?\n",
    "- Data wrangling = process of cleaning, structuring, and transforming raw data into a usable format for analysis.\n",
    "- Data in real life is often incomplete, inconsistent, or messy.\n",
    "- Goal: Make it suitable for machine learning models or analysis.\n",
    "### Steps usually involved in data wrangling:\n",
    "\n",
    "1. Collecting the data (CSV, SQL, API, Excel, etc.)\n",
    "2. Cleaning (handling missing values, removing duplicates, fixing errors)\n",
    "3. Transforming (scaling, normalization, encoding categorical values)\n",
    "4. Feature engineering (extracting new features from existing ones)\n",
    "5. Preparing final dataset for training/testing\n",
    "\n",
    "##### In simple terms: Data Wrangling is like preparing ingredients before cooking a dish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088962a-9b38-4737-ae6b-5a8727af407b",
   "metadata": {},
   "source": [
    "## 2) Overview of Scikit-learn and its Role in Data Science\n",
    "### What is Scikit-learn?\n",
    "- A popular Python library for Machine Learning.\n",
    "- Built on top of\n",
    "    - NumPy,\n",
    "    - Pandas,\n",
    "    - SciPy\n",
    "    - Matplotlib.\n",
    "- Provides ready-to-use tools for\n",
    "    - data preprocessing,\n",
    "    - wrangling,\n",
    "    - model building,\n",
    "    - evaluation, \n",
    "    - deployment.\n",
    "\n",
    "### Why we can use Scikit-learn for Data Wrangling?\n",
    "- Has utilities for\n",
    "    - splitting datasets,\n",
    "    - scaling,\n",
    "    - encoding categorical features,\n",
    "    - feature selection,\n",
    "    - pipelines.\n",
    "- Ensures consistency and reproducibility.\n",
    "- Works seamlessly with Pandas DataFrames and NumPy arrays.\n",
    "##### Think of Scikit-learn as a “Swiss Army Knife” for machine learning and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c751e18-c5bd-4723-a8a9-053e857a1e33",
   "metadata": {},
   "source": [
    "## 3) Playing with Datasets using Scikit-learn Utilities\n",
    "\n",
    "- Scikit-learn provides toy datasets to practice.\n",
    "- Scikit-learn gives us ready-made data for learning data wrangling and ML basics.\n",
    "\n",
    "Example Datasets:\n",
    "- iris → classification dataset of flower species\n",
    "- digits → handwritten digits recognition dataset\n",
    "- wine → wine classification dataset\n",
    "- breast_cancer → medical classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089ec5b1-d9e8-422c-8589-9c1d02fb06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbe491-233d-41f1-90d5-b91429cbc182",
   "metadata": {},
   "source": [
    "##### Let's Load iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9d1d82-9885-4218-a7b7-75ef660b346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b3e2c-7166-48b1-b4d4-a1356324b223",
   "metadata": {},
   "source": [
    "##### Dataset features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a38b3e6d-dff7-4e11-9c61-9d9712ce9220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (150, 4)\n",
      "Target shape (150,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features shape:\", iris.data.shape)\n",
    "print(\"Target shape\", iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd9438-aa21-4622-a75f-182ff820a822",
   "metadata": {},
   "source": [
    "##### Display feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b0687f-2416-4204-88c7-5712785a4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96eb4a-a5a7-4ce0-ad5b-50b47dbbe6a3",
   "metadata": {},
   "source": [
    "##### First 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2436d0-0ffe-4855-9d28-d8a0c40a86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample date:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample date:\\n\",iris.data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24a59f-7705-49ac-aadd-e6ded4c9ea7c",
   "metadata": {},
   "source": [
    "### 1st summary\n",
    "1) Data Wrangling = preparing data for analysis from messy data.\n",
    "\n",
    "2) Scikit-learn helps automate preprocessing & ML tasks.\n",
    "\n",
    "3) Scikit-learn provides toy datasets to practice wrangling before working on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a59c18-4ad3-452a-b09a-fa89c55e080b",
   "metadata": {},
   "source": [
    "# Understanding Classes in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d41f4-4fe8-42f5-936c-0ce7dbd41a30",
   "metadata": {},
   "source": [
    "## 4) Scikit-learn API Design Principles\n",
    "\n",
    "Scikit-learn has a consistent API design, which makes it very beginner-friendly.\n",
    "\n",
    "### Key principles:\n",
    "1) Consistency\n",
    "    - All objects share the same basic API:\n",
    "    - .fit() → train/learn from data\n",
    "    - .transform() → modify/transform data\n",
    "    - .predict() → make predictions\n",
    "\n",
    "2) Simplicity\n",
    "\n",
    "    - Few main interfaces:\n",
    "        - estimators,\n",
    "        - transformers,\n",
    "        - predictors,\n",
    "        - pipelines.\n",
    "\n",
    "3) Composability\n",
    "\n",
    "    - Different parts can be combined together\n",
    "    - Example: preprocessing + model inside a pipeline\n",
    "\n",
    "**Analogy:**\n",
    "- Think of Scikit-learn like LEGO blocks – each block (class) has a specific role, and you can connect them to build bigger systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c6e4a-b346-4b2e-9e8d-e7134e9895b1",
   "metadata": {},
   "source": [
    "## 5) Classes and Objects in Scikit-learn\n",
    "- Classes = blueprints for models or preprocessing steps.\n",
    "- Objects = instances of those classes.\n",
    "\n",
    "Let's see Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36cf96ed-f835-48ce-9d70-c3877cd6fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._base.LinearRegression'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "reg = LinearRegression()\n",
    "print(type(reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99421908-2b92-462d-98d4-d3f354f0049d",
   "metadata": {},
   "source": [
    "Here LinearRegression is a *class*, and reg is the *object*.\n",
    "- **Class** -> LinearRegression\n",
    "- **Object** -> reg (instance of LinearRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b6d24-9075-45a5-a100-c95dc4378a24",
   "metadata": {},
   "source": [
    "## 6) Working with Estimators, Transformers, and Pipelines\n",
    "- Estimator → learns from data (fit, predict)\n",
    "- Transformer → transforms data (fit, transform)\n",
    "- Pipeline → chain of transformers + estimator (makes workflows easier & reproducible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9a3e2-6c7a-4641-9d0c-dac55c8c7ce8",
   "metadata": {},
   "source": [
    "### A. Estimators\n",
    "An estimator is any object in Scikit-learn that can learn from data.\n",
    "- It has a .fit() method to train on data.\n",
    "- If it’s a model, it often has .predict() to make predictions.\n",
    "- Examples:\n",
    "    - LinearRegression()\n",
    "    - LogisticRegression()\n",
    "    - KMeans()\n",
    "    - DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e60627e6-7b18-4390-aa4a-75baa9871cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 5: [10.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])   # features\n",
    "Y = np.array([2,4,6,8])              # targeting\n",
    "\n",
    "model = LinearRegression()           # Estimator\n",
    "model.fit(X,Y)                       # Training\n",
    "\n",
    "print(\"Prediction for 5:\", model.predict([[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66be29-76ab-4052-964b-71a46aa03df0",
   "metadata": {},
   "source": [
    "**Analogy** to understand **Estimators**\n",
    "- Think of an estimator as a teacher who learns patterns from student data (input X and output y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e369cd-f83d-4959-b085-dede42d31826",
   "metadata": {},
   "source": [
    "### B. Transformers\n",
    "- A transformer is an object that transforms data (changes its representation).\n",
    "- It has a .fit() method (to learn parameters)\n",
    "- and a .transform() method (to apply transformation).\n",
    "- Examples:\n",
    "    - StandardScaler() → scales features\n",
    "    - MinMaxScaler() → normalizes values between 0 and 1\n",
    "    - OneHotEncoder() → converts categories into numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68fb1560-225e-4362-bb0a-87509fb513a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data:\n",
      " [[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()     # Transformer\n",
    "\n",
    "X = [[1.0],[2.0],[3.0],[4.0],[3.0]]\n",
    "scaler.fit(X)\n",
    "print(\"Transformed data:\\n\", scaler.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c54b22-e743-4154-9ebd-e6881f58dd19",
   "metadata": {},
   "source": [
    "**Analogy** to understand **Transformers**\n",
    "- A transformer is like a chef who prepares raw vegetables (raw data) into a clean, usable dish (processed data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eda875-9bfb-43c6-ab8e-45905227a164",
   "metadata": {},
   "source": [
    "### C. Pipelines\n",
    "- A pipeline is a sequence of transformers followed by an estimator.\n",
    "    - First: transformers prepare/clean the data\n",
    "    - Last: estimator trains/predicts\n",
    "- Examples:\n",
    "    - Scale → Encode → Train Model\n",
    "    - Clean Data → Feature Selection → Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "edb4383e-d718-47a9-9e5b-fcfa67653f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),            # transformer\n",
    "    ('classifier', LogisticRegression())     # estimator\n",
    "])\n",
    "\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b44314-ffcd-4db1-b54b-d175ab753396",
   "metadata": {},
   "source": [
    "**Analogy** to understand **pipelines**\n",
    "\n",
    "- A pipeline is like an assembly line in a factory. Raw materials (data) pass through several steps (transformers) and finally become a product (predictions from estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707df92-0a48-45d7-8dc2-67276e88a7f6",
   "metadata": {},
   "source": [
    "### 2nd summary\n",
    "4) The Scikit-learn API is consistent, with most classes using .fit(), .transform(), and .predict() methods.\n",
    "5) In Scikit-learn, classes act as blueprints while objects are their working instances.\n",
    "6) An estimator learns from data, a transformer modifies data, and a pipeline links them together into a simple workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fcec5-e855-4532-b260-1eaecbb80845",
   "metadata": {},
   "source": [
    "# Defining Applications for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fcce19-70b3-4b58-9859-013798ee5f9b",
   "metadata": {},
   "source": [
    "## 7) Use Cases of Scikit-learn in Data Wrangling\n",
    "\n",
    "- Scikit-learn is not only for machine learning models but also widely used for data preparation.\n",
    "- Key Use Cases:\n",
    "    - Handling Missing Data → SimpleImputer, KNNImputer\n",
    "    - Scaling & Normalization → StandardScaler, MinMaxScaler\n",
    "    - Encoding Categorical Features → OneHotEncoder, LabelEncoder\n",
    "    - Feature Selection → SelectKBest, VarianceThreshold\n",
    "    - Splitting Data → train_test_split\n",
    "    - Pipelines for Automation → chaining preprocessing + model training\n",
    "\n",
    "**Analogy** to understand **Scikit-learn**\n",
    "- Think of Scikit-learn like a toolbox for carpenters – it has all the essential tools to prepare raw wood (raw data) into usable furniture (ML-ready dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de52a11-6135-46b6-a530-b6c1fa0c1e14",
   "metadata": {},
   "source": [
    "## 8) Feature Extraction, Preprocessing, and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376530",
   "metadata": {},
   "source": [
    "### 1. Feature Extraction\n",
    "\n",
    "- Turning raw data into structured features.\n",
    "\n",
    "- Example:\n",
    "    - From text → word counts, TF-IDF, hashing trick\n",
    "    - From images → pixel values, edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['data' 'learn' 'love' 'science' 'scikit' 'with' 'wrangling']\n",
      "Vectorized Data:\n",
      " [[1 0 1 1 0 0 0]\n",
      " [1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"I love data science\", \"Data wrangling with scikit-learn\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Vectorized Data:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63eb5d3",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "- Preparing raw data so models can understand it.\n",
    "- Examples: scaling, normalization, encoding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f87337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data: [[-1.22474487]\n",
      " [ 0.        ]\n",
      " [ 1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [[10], [20], [30]]\n",
    "scaler = StandardScaler()\n",
    "print(\"Scaled Data:\", scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d93fb",
   "metadata": {},
   "source": [
    "### 3. Transformation\n",
    "- Converting features into a different representation.\n",
    "- Example: PolynomialFeatures → generate higher-order features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee5a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data:\n",
      " [[1. 2.]\n",
      " [1. 3.]\n",
      " [1. 4.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[2], [3], [4]])\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "print(\"Transformed Data:\\n\", poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70672a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data:\n",
      " [[ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[2], [3], [4]])\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "print(\"Transformed Data:\\n\", poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51006e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data:\n",
      " [[ 1.  2.  4.  8.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  4. 16. 64.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[2], [3], [4]])\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "print(\"Transformed Data:\\n\", poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8e6f8",
   "metadata": {},
   "source": [
    "## 9) Practical Mini-Projects (Do it by Yourself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d94fab",
   "metadata": {},
   "source": [
    "- Scikit-learn provides powerful tools for data wrangling before ML.\n",
    "\n",
    "- Feature extraction = converting raw input into useful features.\n",
    "\n",
    "- Preprocessing & transformation make data clean and standardized.\n",
    "\n",
    "- Mini-projects (text, numeric, categorical) show real-life applications of wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3e55d",
   "metadata": {},
   "source": [
    "#### A. Text Data (NLP Example)\n",
    "\n",
    "- Problem: Sentiment classification (positive/negative reviews).\n",
    "    - Step: Use CountVectorizer or TfidfVectorizer → transform text → train a classifier.\n",
    "    \n",
    "#### B. Numeric Data (Regression Example)\n",
    "- Problem: Predict house prices.\n",
    "    - Step: Scale numeric data (StandardScaler) → train regression model.\n",
    "\n",
    "#### C. Categorical Data (Classification Example)\n",
    "- Problem: Predict if a customer will buy a product.\n",
    "    - Step: Encode categories (OneHotEncoder) → train LogisticRegression.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42756e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
